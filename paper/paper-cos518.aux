\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{netflow}
\citation{samplehold}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Graph of the cumulative traffic addressed to/from the top $k$ sources/destinations, captured from an ISP backbone link.\relax }}{1}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:cumulative}{{1}{1}{Graph of the cumulative traffic addressed to/from the top $k$ sources/destinations, captured from an ISP backbone link.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{1}{section.2}}
\newlabel{sec:related}{{2}{1}{Related Work}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Sampling Algorithms}{1}{subsection.2.1}}
\citation{countmin}
\citation{spacesaving}
\citation{rap}
\citation{hashpipe}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Sketch Algorithms}{2}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Counter Based Algorithms}{2}{subsection.2.3}}
\newlabel{fig:alg1}{{\caption@xref {fig:alg1}{ on input line 16}}{2}{Counter Based Algorithms}{figure.caption.3}{}}
\citation{hashpipe}
\newlabel{fig:alg2}{{\caption@xref {fig:alg2}{ on input line 22}}{3}{Counter Based Algorithms}{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The minimum of a randomly selected set of elements approaches the true minimum.\relax }}{3}{figure.caption.5}}
\newlabel{fig:probMin}{{2}{3}{The minimum of a randomly selected set of elements approaches the true minimum.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Design}{3}{section.3}}
\newlabel{sec:design}{{3}{3}{Design}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Probabilistic Minimum}{3}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Randomized Assignment}{3}{subsection.3.2}}
\citation{p4lang}
\citation{p4code}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Logarithmic Admission Policy}{4}{subsection.3.3}}
\newlabel{fig:bp-image}{{\caption@xref {fig:bp-image}{ on input line 22}}{4}{Logarithmic Admission Policy}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Prototype in P4}{4}{section.4}}
\newlabel{sec:prototype}{{4}{4}{Prototype in P4}{section.4}{}}
\citation{mininet}
\citation{caida}
\@writefile{toc}{\contentsline {section}{\numberline {5}Evaluation}{5}{section.5}}
\newlabel{sec:eval}{{5}{5}{Evaluation}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Accuracy Metrics}{5}{subsection.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Comparison of Bouncer admission policy algorithm with different combinations of randomization and hashing at each stage. In hashPipeBouncer-nonRandom, consistent hash functions are used in every stage. The hashPipeBouncer algorithm only applies randomness in the first stage. This variation performed best across all memory sizes and is the main Bouncer algorithm we use in further comparison. The hashPipeRandomBouncer algorithm uses randomness in all stages.\relax }}{6}{figure.caption.7}}
\newlabel{fig:bouncer}{{3}{6}{Comparison of Bouncer admission policy algorithm with different combinations of randomization and hashing at each stage. In hashPipeBouncer-nonRandom, consistent hash functions are used in every stage. The hashPipeBouncer algorithm only applies randomness in the first stage. This variation performed best across all memory sizes and is the main Bouncer algorithm we use in further comparison. The hashPipeRandomBouncer algorithm uses randomness in all stages.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Impact of random assignment versus hash functions. The hashPipeRandom algorithm randomizes indices in all stages and provides some benefits at high memory sizes. The hashPipefRand algorithm assigns a random index in the first stage and then uses consistent hash functions in the following stages. Randomization in the first stage improves accuracy rates across the entire tested memory range, showing that randomization is useful absent any strict admission policy.\relax }}{6}{figure.caption.8}}
\newlabel{fig:interview}{{4}{6}{Impact of random assignment versus hash functions. The hashPipeRandom algorithm randomizes indices in all stages and provides some benefits at high memory sizes. The hashPipefRand algorithm assigns a random index in the first stage and then uses consistent hash functions in the following stages. Randomization in the first stage improves accuracy rates across the entire tested memory range, showing that randomization is useful absent any strict admission policy.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Comparison of HashPipe Implementations}{6}{subsection.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Impact of random slot assignment versus hash functions. The hashPipeRandom algorithm randomizes indices in all stages and provides some benefits at high memory sizes. The hashPipefRand algorithm only hashes to a random index in the first stage and then uses consistent hash functions in the following stages. Randomization in the first stage improves accuracy rates across the entire tested memory range, showing that randomization is useful absent any strict admission policy.\relax }}{7}{figure.caption.9}}
\newlabel{fig:random}{{5}{7}{Impact of random slot assignment versus hash functions. The hashPipeRandom algorithm randomizes indices in all stages and provides some benefits at high memory sizes. The hashPipefRand algorithm only hashes to a random index in the first stage and then uses consistent hash functions in the following stages. Randomization in the first stage improves accuracy rates across the entire tested memory range, showing that randomization is useful absent any strict admission policy.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Impact of varying the factor f when calculating the admission threshold $p = 1 / (f*log(c_m + 1))$. Results show that factors of 5 and greater experienced relatively similar accuracy rates for both front rejection and back rejection.\relax }}{7}{figure.caption.10}}
\newlabel{fig:log}{{6}{7}{Impact of varying the factor f when calculating the admission threshold $p = 1 / (f*log(c_m + 1))$. Results show that factors of 5 and greater experienced relatively similar accuracy rates for both front rejection and back rejection.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Comparison of false negatives of Interview and Bouncer to baseline. Both algorithm optimizations improve on the standard HashPipe algorithm over the entire tested memory range.\relax }}{7}{figure.caption.10}}
\newlabel{fig:falsenegatives}{{7}{7}{Comparison of false negatives of Interview and Bouncer to baseline. Both algorithm optimizations improve on the standard HashPipe algorithm over the entire tested memory range.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Space Saving Comparison}{7}{subsection.5.3}}
\bibdata{local}
\bibcite{rap}{{1}{2017}{{Ben~Basat}}{{}}}
\bibcite{p4lang}{{2}{2016}{{Consortium}}{{}}}
\bibcite{countmin}{{3}{2003}{{Cormode and Muthukrishnan}}{{}}}
\bibcite{samplehold}{{4}{2003}{{Estan and Varghese}}{{}}}
\bibcite{caida}{{5}{2017}{{for Applied Internet Data~Analysis}}{{}}}
\bibcite{spacesaving}{{6}{2005}{{Metwally and El~Abbadi}}{{}}}
\bibcite{netflow}{{7}{}{{Netflow}}{{}}}
\bibcite{hashpipe}{{8}{2017}{{Sivaraman and Rexford}}{{}}}
\bibcite{p4code}{{9}{2016}{{Sivaraman}}{{}}}
\bibcite{mininet}{{10}{}{{Team}}{{}}}
\bibstyle{abbrvnat}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Comparison of algorithms when number of table stages is varied. Interview and Bouncer do not suffer from the same accuracy drawbacks as the baseline when increasing the number of table stages.\relax }}{8}{figure.caption.11}}
\newlabel{fig:stageComparison}{{8}{8}{Comparison of algorithms when number of table stages is varied. Interview and Bouncer do not suffer from the same accuracy drawbacks as the baseline when increasing the number of table stages.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Comparison of Interview and Bouncer with Space Saving with number of counters varied.\relax }}{8}{figure.caption.12}}
\newlabel{fig:spacesaver}{{9}{8}{Comparison of Interview and Bouncer with Space Saving with number of counters varied.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusions}{8}{section.6}}
\newlabel{sec:conclusion}{{6}{8}{Conclusions}{section.6}{}}
