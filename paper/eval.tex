\section{Evaluation}
\label{sec:eval}

We evaluate the accuracy of the variants of our algorithm through a series of simulations in which we fine tune the various parameters: number of stages, memory size, and randomized admission policy coefficients. In order to simulate realistic streams of traffic, we ran testing using three different traces from the equinox chicago ISP backbone link, recorded in 2016. These anonymized traces each contain between 20 - 40 million packets, over 1 million different flows, and range from 40 minutes to 1 hour long. The data was obtained with permission from the Center for Applied Internet Data Analysis (CAIDA). We parsed the data from the CAIDA traces to isolate only the source IP addresses. Each source IP address is considered a separate flow.

\subsection{Accuracy Metrics}
Through measuring the false negative rate when testing our algorithms against the CAIDA data, we attempted to experimentally determine which combination of design policies yielded the best results. We focused on testing the two different admission policies, and applied different levels of randomized ``hashing'' to find an optimal algorithm. When applying front rejection, we found that uniformly randomizing the table index in the first stage only yielded the best results, and gave as much as a 50 percent accuracy improvement over simple front rejection with normal hashing (see Figure X). While some randomization provided improvements for front rejection, our tests of back rejection showed that normal hashing at all stages was best (see Figure X).

Why is there a discrepancy in the effectiveness of random hashing? Perhaps this can be explained by the increased level of eviction that occurs in the first stage when random indices are used. When hashes are randomized, it is less likely that the same key will end up at the same index and increase its count in the first stage. So effectively, keys are more quickly pushed to later stages where there is a greater emphasis on retaining heavy flows. Since consistent hashing is performed in later stages, the same key is able to add to its count, and can do this more frequently when it is quickly sent down the pipeline from the first stage. Randomized hashing more effectively treats the first stage as transient, so light flows will more quickly be evicted and heavy flows will more quickly be retained in the core of the pipeline. For this reason, randomizing indices in all stages is less effective because retention also is lower in the latter stages, when it is most important to hold onto heavy flows. Randomization in the first stage adds an accuracy boost to front rejection, but it introduces problems with the more complex back rejection policy. The goal with the Interview policy, is to deny first stage admission to the incoming key at a high rate, and replace it with the probabilistic minimum that emerges from the pipeline. However, when randomization is applied, we can't guarantee that we are swapping the minimum with the incoming key in the first stage. This renders our stricter admission policy ineffective if we can't guarantee that the incoming key is the one being evicted in the first stage. Evidently, randomized hashing  in the first stage can effectively increase accuracy under the right conditions, and was even shown to be useful as a standalone improvement to HashPipe without applying front or back rejection (see Figure X). However, front rejection does not benefit from randomized hashing, and the standalone version of the policy ended up with the best performance.

We were also able to tune our algorithm's performance by experimenting with the multiplicative factor in our logarithmic admission equation. Figure X shows how performance was best at values between 1 and 10, so we settled on a factor of 5 for all future experimentation.



\subsection{Comparison of HashPipe Implementations}
We began algorithm comparison by varying the available memory size, which corresponds to a greater number of counters that can be stored across all hash tables. Figure 2 shows our results when searching for the top 300 flows and using 6 table stages. Both algorithms that applied an admission policy outperformed the standard HashPipe algorithm for all tested memory sizes. Accuracy rates improved by about 20 percent when the number of counters was limited to 300, equal to the number of flows top-k flows being identified, and increases to more than 50 percent improvement when more than 2000 counters are used. HashPipe with front rejection in particular brought its accuracy rate to more than 90 percent with 2000 counters, which is less than half the memory required by the standard HashPipe algorithm to achieve that accuracy with 300 heavy hitters and 6 table stages. In all algorithms, improvement begins to diminish after 1000 counters are available.

In addition to testing the impact of varying memory, we also experimented with the number of table stages available. HashPipe tends to perform best with a limited amount of stages so that the number of duplicates is limited. The optimal number of stages is about 6 with the standard HashPipe, but Figure 3 shows that our algorithms continue to improve past this point as the number of table stages is increased. While Interview and Bouncer also experience diminishing returns after about 8 table stages, they do not see the same performance decrease present in the standard HashPipe algorithm.

\subsection{Space Saver Comparison}
As a final benchmark, we compare our algorithms to the Space Saver algorithm, which finds the true minimum of all of its counters. While this algorithm allows for more accuracy, hardware constraints that restrict multiple reads to the same table make implementations of this algorithm unrealistic. Figure X shows that our algorithms fall behind an idealized implementation of Space Saver, where it is no issue to compute a global minimum. Similar to the baseline implementation of HashPipe, our implementations also outperform Space Saver when a low amount of memory is available. This is because Space Saver is only guaranteed to hold onto the kth heaviest item when it is larger than average count in the table, so its full benefits are realized when memory is increased


\begin{figure}[t]
  \centering
    \includegraphics[scale=0.5]{bouncer}
     \caption{Comparison of Bouncer admission policy algorithm with different levels of random hashing. In hashPipeBouncer-nonRandom, consistent hash functions are used in every stage, and no randomnes is applied. The hashPipeBouncer algorithm, only applies randomness in the first stage--should an incoming packet be admitted to the table, it is placed at a random index rather than using a hash function. This variation performed the best across all tested memory sizes and is the main Bouncer algorithm we use in further comparison. The hashPipeRandomBouncer algorithm does comparisons and inserts at random indices in all table stages, and was also shown to be outperformed by limiting randomization to the first stage.}
     \label{fig:bp-image}
\end{figure}

\begin{figure}[t]
  \centering
    \includegraphics[scale=0.5]{interview}
     \caption{Impact of random hashing rather than using hash functions. The hashPipeRandom algorithm randomizing indices in all stages and provides some benefits at high memory sizes. The hashPipefRand algorithm only hashes to a random index in the first stage and then uses consistent hash functions in the following stages. Randomization in the first stage improves accuracy rates across the entire tested memory range, showing that randomization is useful absent any strict admission policy.}
\end{figure}

\begin{figure}[t]
  \centering
    \includegraphics[scale=0.5]{random}
     \caption{Impact of random hashing rather than using hash functions. The hashPipeRandom algorithm randomizing indices in all stages and provides some benefits at high memory sizes. The hashPipefRand algorithm only hashes to a random index in the first stage and then uses consistent hash functions in the following stages. Randomization in the first stage improves accuracy rates across the entire tested memory range, showing that randomization is useful absent any strict admission policy.}
     \label{fig:bp-image}
\end{figure}

\begin{figure}[t]
  \centering
    \includegraphics[scale=0.5]{log}
     \caption{Impact of varying the factor f when calculating the admission threshold $p = 1 / (f*log(c_m + 1))$. Results show that factors of 5 and greater experienced relatively similar accuracy rates for both front rejection and back rejection.}
     \label{fig:bp-image}
\end{figure}

\begin{figure}[t]
  \centering
    \includegraphics[scale=0.5]{falsenegatives}
     \caption{Comparison of false negatives of Interview and Bouncer to baseline. Both algorithm optimizations improve on the standard HashPipe algorithm over the entire tested memory range}
     \label{fig:bp-image}
\end{figure}

\begin{figure}[t]
  \centering
    \includegraphics[scale=0.5]{stageComparison}
     \caption{Comparison of algorithms when number of table stages is varied. Interview and Bouncer do not suffer from the same accuracy drawbacks as the baseline when increasing the number of table stages}
     \label{fig:bp-image}
\end{figure}

\begin{figure}[t]
  \centering
    \includegraphics[scale=0.45]{spacesaver}
     \caption{Comparison of Interview and Bouncer with Space Saver}
     \label{fig:bp-image}
\end{figure}